{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32ea4e-a8d4-4b13-8518-c575cdf183d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from ast import literal_eval\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bedecc0-e54e-4984-a880-0853f462adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "parent = str(Path(cwd).parents[0])\n",
    "sys.path.append(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5fdfe-6463-43e9-9cab-159d2bbd471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing custom functions for analysis\n",
    "\n",
    "from lib.functions import status_exclude, group_dates, date_fix\n",
    "\n",
    "flowchart_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3379a-f5ce-4a5b-b86f-8bdfddfd0c89",
   "metadata": {},
   "source": [
    "The full scrape of the EUCTR from December 2020 is too big to easily committ to GitHub (>200mb). A copy of the full file lives on the OSF repo (https://osf.io/tu5pz). For our purposes in this analysis, I've created a smaller version of that file with only the columns we need. The below cell will check to see if that smaller file exists and will try to create it if not. However, by default it will throw an error if it tries to create it since the bigger original file isn't in this repo. If you want to work with, or check, the raw file directly (called `euctr_euctr_dump-2020-12-03-095517.csv.zip`), you can download the zipped csv from the OSF link above and put it in the `source_data` folder. I've also split it up into 4 small files that I can committ in the `source_data` folder which you can play around with individually or combine to re-make the original dataset if you'd like. Using something like `pd.concat` to do this is computationally expensive and annoying so I avoid that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25433be2-17e4-441b-bc0c-7eb505df9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = ['eudract_number', \n",
    "           'eudract_number_with_country', \n",
    "           'end_of_trial_status', \n",
    "           'trial_results', \n",
    "           'date_of_competent_authority_decision', \n",
    "           'date_of_ethics_committee_opinion', \n",
    "           'trial_in_the_member_state_concerned_years', \n",
    "           'trial_in_all_countries_concerned_by_the_trial_years', \n",
    "           'trial_in_the_member_state_concerned_months', \n",
    "           'trial_in_all_countries_concerned_by_the_trial_months', \n",
    "           'trial_in_all_countries_concerned_by_the_trial_days', \n",
    "           'trial_in_the_member_state_concerned_days', \n",
    "           'date_of_the_global_end_of_the_trial']\n",
    "\n",
    "try:\n",
    "    dec_full = pd.read_csv(parent + '/data/source_data/' + 'euctr_processed_dec2020.csv.zip', low_memory=False, usecols=usecols)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    dec_full = pd.read_csv(parent + '/data/source_data/' + 'euctr_euctr_dump-2020-12-03-095517.csv.zip', low_memory=False, usecols=usecols)\n",
    "    \n",
    "    dec_full.to_csv(parent + '/data/source_data/' + 'euctr_processed_dec2020.csv.zip', index=False, compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371eec1-ab24-48b6-9b71-0668f39e2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This loads in the results section scrape\n",
    "dec_results = pd.read_csv(parent + '/data/source_data/' + 'euctr_data_quality_results_scrape_dec_2020.csv.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e57cce-249b-474d-bdba-d023aca986d9",
   "metadata": {},
   "source": [
    "First, we can quickly exclude all trials that appear to have never started in Europe because they were either \"Not Authorised\" or \"Prohibited by CA\" across all trial protocols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f440a57-8952-4070-8653-e87770565766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the columns we need at the moment\n",
    "#Applying custom imported function during groupby to combine multiple country protocols into 1 record and exclude trials\n",
    "#In a status we don't want\n",
    "\n",
    "trial_status = dec_full[['eudract_number', \n",
    "                         'eudract_number_with_country', \n",
    "                         'end_of_trial_status', \n",
    "                         'trial_results']].groupby('eudract_number').apply(status_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d77db96-153f-4b31-9b36-3f3d9bfd2a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing exclusions of the status\n",
    "\n",
    "trial_status['never_started'] = np.where(trial_status.other_status == trial_status.number_of_countries, 1, 0)\n",
    "never_started_exclusions = trial_status[trial_status.never_started == 1].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceaafcc-650c-4166-9130-3c7f6a37342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets exclude those trial moving forward:\n",
    "\n",
    "dec_started = dec_full[~dec_full.eudract_number.isin(never_started_exclusions)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c188d-2be1-4104-97ff-bc8151647c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the flow chart lets record the amount of unique EUCTR numbers and those excluded for not starting.\n",
    "print(f'There are {dec_full.eudract_number.nunique()} unique trials on the EUCTR. {len(never_started_exclusions)} of these were excluded for never being authorised.')\n",
    "\n",
    "flowchart_dict['full_euctr'] = dec_full.eudract_number.nunique()\n",
    "flowchart_dict['not_authorised'] = len(never_started_exclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1789900-0356-48d2-babc-dc53bf08a0db",
   "metadata": {},
   "source": [
    "# Extracted End Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d54fd-ca8f-4851-bc2d-365b10f3a9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First some data loading and housekeeping\n",
    "\n",
    "#Taking only the columns we need from the protocol scrape and then merging in the results completion information.\n",
    "\n",
    "dec_short = dec_started[['eudract_number', 'date_of_the_global_end_of_the_trial']]\n",
    "\n",
    "merged_dates = dec_short.merge(dec_results[['trial_id', 'global_end_of_trial_date']], \n",
    "                               how='left', left_on='eudract_number', right_on='trial_id').drop('trial_id', axis=1)\n",
    "\n",
    "#Renaming the columns and making the dates into dates\n",
    "merged_dates.columns = ['eudract_number', 'protocol_completion', 'results_completion']\n",
    "\n",
    "merged_dates['protocol_completion'] = pd.to_datetime(merged_dates['protocol_completion'])\n",
    "merged_dates['results_completion'] = pd.to_datetime(merged_dates['results_completion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b046eb-9dec-41d4-9b45-9fcb96cbb2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check on the data\n",
    "merged_dates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e04e0-4680-4880-9864-d4aab0f1fcc1",
   "metadata": {},
   "source": [
    "Here we use 2 functions, imported above, to help manage the data.\n",
    "\n",
    "1. **group_dates** lets us collapse each trial into a single trial ID. Each row in this initial dataset reprents a country-level protocol, not an entire trial so some IDs are repeated. Even though the protocol completion date should hypothetically be the same across all the protocols, this is not guaranteed. During the groupby we take the latest completion date provided (or the \"max\"). To keep the dates together, we also take the max of the `results_completion` but this date will be the same across all entries with results. \n",
    "\n",
    "\n",
    "2. **date_fix** gets rid of obvious outlier dates with completion dates either before 2004 (i.e. before the EUCTR was created) or after 2020 (i.e. in the future and therefore should not exist yet since these dates are entered retrospectively). While some of these may not be mistakes, they obviously represent some sort of odd situation we would rather avoid. These are turned into null values where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56709718-f2f3-4739-bb7f-49353c7c6705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running the groupby with group_dates\n",
    "\n",
    "latest_dates = merged_dates.groupby('eudract_number', as_index=False).apply(group_dates).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984a4cd-0d0d-491e-85c3-9d62c204d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking how many dates we would be exluding withe the date_fix function we see it is a nominal amount\n",
    "\n",
    "print(f'{len(latest_dates[(latest_dates[\"latest_completion_p\"] < pd.to_datetime(\"2004-01-01\")) | (latest_dates[\"latest_completion_p\"] > pd.to_datetime(\"2020-12-31\"))])}\\\n",
    " replaced from the protocol dates')\n",
    "\n",
    "print(f'{len(latest_dates[(latest_dates[\"latest_completion_r\"] < pd.to_datetime(\"2004-01-01\")) | (latest_dates[\"latest_completion_r\"] > pd.to_datetime(\"2020-12-31\"))])}\\\n",
    " replaced from the results dates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8843ad6-4108-4ac0-bd99-6ebb081ad565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are interested in these dates you can uncomment and run either of the following:\n",
    "\n",
    "#latest_dates[(latest_dates[\"latest_completion_p\"] < pd.to_datetime(\"2004-01-01\")) | (latest_dates[\"latest_completion_p\"] > pd.to_datetime(\"2020-12-31\"))]\n",
    "\n",
    "#latest_dates[(latest_dates[\"latest_completion_r\"] < pd.to_datetime(\"2004-01-01\")) | (latest_dates[\"latest_completion_r\"] > pd.to_datetime(\"2020-12-31\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c678e38e-85b6-413e-8614-ad91d331b7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actually running the date fix\n",
    "\n",
    "latest_dates['latest_completion_p'] = latest_dates['latest_completion_p'].apply(date_fix)\n",
    "latest_dates['latest_completion_r'] = latest_dates['latest_completion_r'].apply(date_fix)\n",
    "\n",
    "#Now we can take the results completion date when available, and the latest protocol completion date otherwise\n",
    "\n",
    "latest_dates['available_completion'] = np.where(latest_dates.latest_completion_r.notnull(), \n",
    "                                            latest_dates.latest_completion_r, latest_dates.latest_completion_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745fb9e-2a62-4edb-a663-5d20ad442320",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e248d-1e21-4079-8d81-780c36db3458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our final data is all of the trials with an \"available_completion_date\"\n",
    "#This forms the population of trials in which we could extract a clear end date.\n",
    "\n",
    "final_dates = latest_dates[latest_dates.available_completion.notnull()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c036ec8a-0b7f-4276-bfff-118c74e46ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'We could extract end dates for {len(final_dates)} trials.')\n",
    "flowchart_dict['extracted_dates'] = len(final_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aaa239-8d69-4e08-98a4-023b5ff61630",
   "metadata": {},
   "source": [
    "# Creating Inferred End Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e761292a-2e73-4601-b813-c6f2ec03930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is everything we couldn't extract an end date for that we will try and infer a date for.\n",
    "\n",
    "no_completion = latest_dates[latest_dates.available_completion.isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f43947-0904-4b21-8bae-f1c1053852f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use that data to create a new dataset, only with those trial IDs, \n",
    "#and with the columns we need from the original December protocol dataset.\n",
    "\n",
    "inf_fields = ['eudract_number', 'eudract_number_with_country', 'date_of_competent_authority_decision', \n",
    "              'date_of_ethics_committee_opinion', 'trial_in_the_member_state_concerned_years', \n",
    "              'trial_in_all_countries_concerned_by_the_trial_years', \n",
    "              'trial_in_the_member_state_concerned_months', \n",
    "              'trial_in_all_countries_concerned_by_the_trial_months', \n",
    "              'trial_in_all_countries_concerned_by_the_trial_days', \n",
    "              'trial_in_the_member_state_concerned_days']\n",
    "\n",
    "no_comp_inf = dec_full[inf_fields][dec_full[inf_fields].eudract_number.isin(no_completion.eudract_number.to_list())].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da00f7-df5e-470b-97ad-cca115a2dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining the dataset\n",
    "\n",
    "no_comp_inf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02000b6c-7c2e-472a-9379-a3ea58d594db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turning dates into dates.\n",
    "\n",
    "no_comp_inf['date_of_competent_authority_decision'] = pd.to_datetime(no_comp_inf['date_of_competent_authority_decision'])\n",
    "no_comp_inf['date_of_ethics_committee_opinion'] = pd.to_datetime(no_comp_inf['date_of_ethics_committee_opinion'])\n",
    "\n",
    "#Creating a new column for the latest approval date within a protocol, and then doing a groupby, so we get the\n",
    "#Latest approval date for the whole trial which we will use later as a proxy for the start date.\n",
    "\n",
    "no_comp_inf['latest_approval'] = no_comp_inf[['date_of_competent_authority_decision', 'date_of_ethics_committee_opinion']].max(axis=1)\n",
    "\n",
    "latest_approval = no_comp_inf[['eudract_number', 'latest_approval']].groupby('eudract_number', as_index=False).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da612411-1b41-43b0-8373-d09d506e8ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we turn the day/month/year data we have on expected duration into just days (assuming a month is 30 days).\n",
    "#We do this across both the duration expected within country and globally and then take the longest of the two\n",
    "#for each protocol and then once again we group to get the longest possible duration provided\n",
    "\n",
    "no_comp_inf['country_days'] = ((no_comp_inf['trial_in_the_member_state_concerned_years'].fillna(0) * 364) + \n",
    "                               (no_comp_inf['trial_in_the_member_state_concerned_months'].fillna(0) * 30) + \n",
    "                               (no_comp_inf['trial_in_the_member_state_concerned_days'].fillna(0))) \n",
    "\n",
    "no_comp_inf['global_days'] = ((no_comp_inf['trial_in_all_countries_concerned_by_the_trial_years'].fillna(0) * 364) + \n",
    "                              (no_comp_inf['trial_in_all_countries_concerned_by_the_trial_months'].fillna(0) * 30) + \n",
    "                              (no_comp_inf['trial_in_all_countries_concerned_by_the_trial_days'].fillna(0)))\n",
    "\n",
    "no_comp_inf['max_days'] = no_comp_inf[['country_days', 'global_days']].max(axis=1)\n",
    "\n",
    "longest_duration = no_comp_inf[['eudract_number', 'max_days']].groupby('eudract_number', as_index=False).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86a858-49f1-487b-9b22-7969d177ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can merge in the latest approval date and pick out the trials we can infer an end date for\n",
    "inferred_df = latest_approval.merge(longest_duration, how='left', on='eudract_number')\n",
    "\n",
    "#So we exclude trials that had 0 duration (meaning no information in these fields) and no approval dates\n",
    "can_infer = inferred_df[(inferred_df.max_days != 0) & (inferred_df.latest_approval.notnull())].reset_index(drop=True)\n",
    "flowchart_dict['inferred'] = len(can_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4eeb74-acd9-4d31-8847-38da1c3c3256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the trials we exluded here for inspection as needed\n",
    "\n",
    "no_inference = inferred_df[(inferred_df.max_days == 0) | (inferred_df.latest_approval.isnull())].eudract_number.to_list()\n",
    "print(f'We could not extract or infer a date for {len(no_inference)} trials.')\n",
    "\n",
    "flowchart_dict['missing_completion_info'] = len(no_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff200e-5e1b-4682-aadd-9afcc5de53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we add the latest approval to the longest trial duration\n",
    "\n",
    "can_infer['inferred_completion'] = can_infer['latest_approval'] + can_infer.max_days.astype('timedelta64[D]')\n",
    "\n",
    "#Now we conservatively add another year to the inferred completion date per our methods\n",
    "\n",
    "can_infer['inferred_completion_adj'] = can_infer['inferred_completion'] + pd.offsets.DateOffset(months=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d32023-a7b6-4320-a667-e7d62954f40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "can_infer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ef0c3-c2fe-443e-b39e-8c54c6dccfad",
   "metadata": {},
   "source": [
    "# Creating final dataset\n",
    "\n",
    "Now we need to bring everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1636716-10a0-4878-9693-d130e580626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get each trial id only once\n",
    "all_ids = dec_full.eudract_number.unique()\n",
    "\n",
    "#turn that into a DataFrame\n",
    "df = pd.DataFrame(all_ids)\n",
    "df.columns = ['eudract_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28e2e92-3b29-468e-a9c6-0dc48c3d5430",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge in the dates we extracted earlier\n",
    "\n",
    "df1 = df.merge(final_dates[['eudract_number', 'available_completion']], how='left', on='eudract_number')\n",
    "\n",
    "df2 = df1.merge(can_infer[['eudract_number', 'inferred_completion_adj']], how='left', on='eudract_number')\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d2a03-9099-418a-9975-29f57593cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conditions to create the inclusion/exclusion categorical variables\n",
    "\n",
    "#Trials that were \"not authorised\" or \"prohibited\" in all countries\n",
    "never_start = df2.eudract_number.isin(never_started_exclusions)\n",
    "\n",
    "#Trials that didn't have an end date, and didn't have sufficient information to infer one\n",
    "no_inf = df2.eudract_number.isin(no_inference)\n",
    "\n",
    "#Trials that had some kind of completion date information\n",
    "avail_comp = df2.available_completion.notnull()\n",
    "\n",
    "#Trials where we had to infer an end date.\n",
    "infer_comp = df2.inferred_completion_adj.notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574f66e-ee69-484c-a608-f31234bc9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the inclusion/exclusion categorical variable\n",
    "\n",
    "conds = [never_start, no_inf, avail_comp, infer_comp]\n",
    "\n",
    "labels = ['No EU Start', 'Cannot Infer', 'Extracted', 'Inferred']\n",
    "\n",
    "df2['exclusion_status'] = np.select(conds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db447b2-a1ba-4f03-aaab-3bf0f474f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0079a-f6fb-4d2c-92fb-c58686b55351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can take a look at how frequently each category appears which shold match the numbers we grabbed earlier\n",
    "#And are storing in that dictionary as we go along.\n",
    "\n",
    "df2.exclusion_status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5508ebee-0dac-421c-babe-8cd0e177f4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowchart_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d10556-f8c0-4b23-aa35-d0b144e09f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the appropriate final date\n",
    "\n",
    "df2['final_date'] = np.where(df2.available_completion.notnull(), \n",
    "                             df2.available_completion, df2.inferred_completion_adj)\n",
    "\n",
    "#Making a binary variable for inclusion based on being completed for 24 months prior to data extraction\n",
    "\n",
    "df2['date_inclusion'] = np.where(df2.final_date < pd.to_datetime('2018-12-01'), 1, 0)\n",
    "\n",
    "#Making a binary variable for inferred end dates\n",
    "\n",
    "df2['inferred'] = np.where(df2.exclusion_status == 'Inferred', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8141ee40-b7e4-40e5-ad49-5ebdfd714479",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowchart_dict['inferred_date_exclude'] = len(df2[(df2.date_inclusion == 0) & (df2.exclusion_status == 'Inferred')])\n",
    "flowchart_dict['extracted_date_exclude'] = len(df2[(df2.date_inclusion == 0) & (df2.exclusion_status == 'Extracted')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645fa5ec-b8af-4e7d-835f-442ddea0d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowchart_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc2aabf-66d8-48df-a3f0-67ae6813e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the final dataset with only included trials, the end date, and the inferred status\n",
    "\n",
    "final_df = df2[df2.date_inclusion == 1][['eudract_number', 'final_date', 'inferred']].reset_index(drop=True)\n",
    "\n",
    "print(f'There are {len(final_df)} trials in the final population we will sample from.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2685886-3588-40f5-ac5f-9731ddf8556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.inferred.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011be5fa-c1d3-4dfe-b080-67b43c2cb641",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_ids = final_df.eudract_number.to_list()\n",
    "\n",
    "only_included = dec_full[dec_full.eudract_number.isin(inc_ids)].reset_index(drop=True)\n",
    "\n",
    "print(len(only_included))\n",
    "\n",
    "print(only_included.eudract_number.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186b1e67-adf8-4aec-953a-8ec8b039a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When the sample was taken the following code was run to generate a random seed.\n",
    "\n",
    "#from random import randint\n",
    "\n",
    "#print(randint(1,10000))\n",
    "\n",
    "#This produced 7872"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4972b0-eee1-443f-a335-e2b0a3acfca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = final_df.sample(500, random_state=7872)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e9442c-7b74-4702-ac06-ce6db73109c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41f668-a5b5-4dc1-bd7c-ca64848a3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment this to save the file\n",
    "\n",
    "#sample.to_csv(parent + '/data/samples/' + 'euctr_search_sample_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ab3a44-e4f7-482b-ac4f-2ab35e863286",
   "metadata": {},
   "source": [
    "# Replacing problematic trials in the dataset\n",
    "\n",
    "Per protocol, trials that are found to be withdrawn, meaning they never happened, at any time during the project, are to be replaced in the sample and re-searched. I will also replace trials that are clearly still ongoing based on other available information or are no longer available on the public EUCTR to be checked for some reason.\n",
    "\n",
    "To do this, we will take the sample population, exclude the trials from our original sample of 500, and then take a new sample of the remaining. At the final analysis, following all searches, there were 20 trials that need to be replaced and re-searched (19 from the original sample and then one of the replacements had to be replaced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a320ab9-2a6b-44d3-8b56-81bfe1a0dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting a new random seed for the new replacement sample\n",
    "\n",
    "#print(randint(1,10000))\n",
    "\n",
    "#This produced 6377"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b3b3ae-12d7-488b-a872-f0aa68a71c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets remove the original 500 trial sample so we don't get any duplicates\n",
    "replacement_pop = final_df[~(final_df.eudract_number.isin(sample.eudract_number.to_list()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee96311-54c2-4bab-8de8-2841c8e7ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#And take a sample\n",
    "replacement_sample = replacement_pop.sample(20, random_state=6377)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b204d-7f7d-4fa3-bad4-9d518e5b7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment this to save the file\n",
    "\n",
    "#replacement_sample.to_csv(parent + '/data/samples/' + 'replacement_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f123163-56e7-43ab-9325-7f9c3a1cb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_sample.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf686bbd-9e31-4a55-8061-ff37a07071f2",
   "metadata": {},
   "source": [
    "# Data for Regression\n",
    "\n",
    "For the regression dataset we need the following categories: </br>\n",
    "Inclusion Category: Taken from above but already in the analysis notebook </br>\n",
    "Trial start year: Manually extracted dataset </br>\n",
    "Sponsor type: Derived below </br>\n",
    "EU country protocols registered: Derived below </br>\n",
    "Final Enrollment: Manually extracted dataset </br>\n",
    "EU only/Multinational: Manually extracted dataset </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53f0f5-18a1-4dcf-92d0-9b61ec30f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First lets load in our sponsor info scrape from Dec 2020\n",
    "spon_df = pd.read_csv(parent + '/data/source_data/' + 'dec2020_spon_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19b26a-e0ae-4430-af67-436c8525b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can limit everything to only the trials considered for our sample\n",
    "incl_trials = sample.eudract_number.to_list() + replacement_sample.eudract_number.to_list()\n",
    "spon_df_filt = spon_df[spon_df.trial_id.isin(incl_trials)].reset_index(drop=True).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "#We don't want to count non-EU/EEA protocols for that variable\n",
    "spon_df_filt_no3rd = spon_df_filt[spon_df_filt.protocol_country != 'Outside EU/EEA'].reset_index(drop=True)\n",
    "\n",
    "#We can get the count of unique EU country protocols\n",
    "prot_counts = spon_df_filt_no3rd[['trial_id', 'protocol_country']].drop_duplicates().groupby('trial_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afbc16-39cf-48b5-9329-6feaa16a3773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets cross check trials with \"No Data Available\"\n",
    "spon_df_filt[spon_df_filt.sponsor_status == 'No Data Available']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df67ee94-c4e2-4691-a286-ddf23f3c56d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can manually check variation in the \"sponsor_status\" column to see which has mixed information\n",
    "spon_df_filt[['trial_id', 'sponsor_status']].groupby('trial_id').nunique().sort_values(by='sponsor_status', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42cd0ba-cdea-4416-a699-a75dc0e6c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this cell to check all these\n",
    "spon_df_filt[spon_df_filt.trial_id == '2006-000666-37']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2c068-a383-4efe-9063-f5c0a0117474",
   "metadata": {},
   "source": [
    "Manual Sponsor Status Check: <br />\n",
    "2007-004805-80: One blank, one commercial. Make commercial. <br />\n",
    "2006-000666-37: Commercial in Germany but non-commercial in GB. On manual check sponsor IATEC B.V. is/was a commercial entity (a CRO).  <br />\n",
    "2012-000347-28: One blank, rest commercial. Make commercial.  <br />\n",
    "2012-001956-20: Commercial all locations except Hungary, however clearly a Commercial entity. <br />\n",
    "2011-000291-34: Truly just a blank sponsor name so unknown <br />\n",
    "2007-003461-41: Deutsches Herzzentrum Berlin is a non-commercial sponsor <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5510f-f288-4847-adac-d1b3ff8c3f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doing the Groupby\n",
    "spon_status = spon_df_filt[['trial_id', 'sponsor_status']].groupby('trial_id').max()\n",
    "\n",
    "print(spon_status.sponsor_status.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0fb11e-fa83-44d3-87f3-d8ada6482185",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing the manually checked data.\n",
    "\n",
    "spon_status.loc['2007-004805-80', 'sponsor_status'] = 'Commercial'\n",
    "spon_status.loc['2006-000666-37', 'sponsor_status'] = 'Commercial'\n",
    "spon_status.loc['2012-000347-28', 'sponsor_status'] = 'Commercial'\n",
    "spon_status.loc['2012-001956-20', 'sponsor_status'] = 'Commercial'\n",
    "spon_status.loc['2011-000291-34', 'sponsor_status'] = 'Unknown'\n",
    "spon_status.loc['2007-003461-41', 'sponsor_status'] = 'Non-Commercial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db176218-26cb-4e80-ba98-70eed27ac971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spon_status.sponsor_status.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d993df-a1d3-47cd-bc76-389c605c5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df = spon_status.join(prot_counts).fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff39e6-c8b4-414d-baab-ec2aeaa3cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_df.to_csv(parent + '/data/additional_data/' + 'reg_spon_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed079825-34fb-4b5e-a258-4cd165ca55bd",
   "metadata": {},
   "source": [
    "# Sponsor Country\n",
    "\n",
    "Each trial will be assigned a “sponsor country” based on the most frequent sponsor country assigned in the EUCTR country protocols. A protocol of a specific country need not contain a sponsor from that country. If no single country appears most frequently, the trial will be coded as having “multi-country” sponsorship. The percent of trials reported to the EUCTR, other registries, and the literature will be reported for each unique sponsor country in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df747e-2f59-4de5-ac59-e8de7600cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = spon_df_filt[['trial_id', \n",
    "                        'sponsor_country']].groupby('trial_id')['sponsor_country'].apply(pd.Series.mode).to_frame().reset_index()\n",
    "\n",
    "multi_country = grouped[grouped.level_1 == 1].trial_id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5fafd1-108c-4361-9e47-1ab9e196853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets check for any that are tied with \"No Data Available\"\n",
    "grouped[grouped.trial_id.isin(multi_country)]\n",
    "#There is one we need to correct for later: 2007-004805-80 needs to be \"United Kingdom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc7c725-6cfb-4b29-9edb-8439462826ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_join = grouped[~grouped.trial_id.isin(multi_country)]\n",
    "\n",
    "final_df = reg_df.merge(to_join, on='trial_id', how='left').drop('level_1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd231b-af24-4f9b-89c7-b952a49d37eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting that data from earlier\n",
    "final_df.loc[final_df[final_df.trial_id == '2007-004805-80'].index[0], 'sponsor_country'] = 'United Kingdom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fc680f-477d-4755-8cf9-e1e651885008",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['sponsor_country'] = final_df['sponsor_country'].fillna('Multi-country')\n",
    "\n",
    "final_df = final_df.replace('France, Metropolitan', 'France')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f367d-d6ca-418c-88d1-79bcb29ddf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[final_df.sponsor_country == 'No Data Available']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da245e54-6692-462a-a60f-67bcc3326663",
   "metadata": {},
   "source": [
    "Manual changes:<br />\n",
    "2010-020521-40: Sponsor is \"EPIFARMA S.R.L.\" which is clearly Italian<br />\n",
    "2011-000291-34: Has no sponsor<br />\n",
    "2013-001103-36: Sponsor is \"Tayside Medical Sciences Centre on behalf of University of Dundee & NHS Tayside\" which is a UK sponsor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee5be3-c045-4f0b-ab9f-2f83a0b2e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.loc[final_df[final_df.trial_id == '2010-020521-40'].index[0], 'sponsor_country'] = 'Italy'\n",
    "final_df.loc[final_df[final_df.trial_id == '2013-001103-36'].index[0], 'sponsor_country'] = 'United Kingdom'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245741ce-e212-48bc-966d-cf83f4e4447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1a139-4ebc-4317-b5d4-2f8e587d19e6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#final_df.to_csv(parent + '/data/additional_data/' + 'spon_country_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b695daa-fd49-4bf2-8d13-7c55282ff023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf21275-3766-4686-b5f7-95f97c13dedf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4ada9-dbdf-4c2c-b4c8-5d77a732874a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1b7f0-8d86-4824-b418-44578363aa29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27715c86-d61f-48cb-a075-f42cb13518a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7d1ba8-a5cd-4c7f-a30e-105191f6b27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d061bf7-cd6c-43ea-b026-007ebe87bd07",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "notebook_metadata_filter": "all,-language_info"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
